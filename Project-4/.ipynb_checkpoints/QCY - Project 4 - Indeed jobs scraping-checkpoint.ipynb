{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from textacy.preprocess import preprocess_text\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.indeed.com/advanced_search?q=title%3A%28data+scientist%29+%24100%2C000+-+%24110%2C000&l=United+States&radius=0&limit=50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start the WebDriver\n",
    "wd = webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125000.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 1057.95562005  seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# list of job categories to search\n",
    "ttl_list = ['data scientist', 'data analyst', 'research scientist', 'business intelligence']\n",
    "\n",
    "# list of salary ranges to search\n",
    "sal_list = []\n",
    "for i in range(0, 300000, 20000):\n",
    "    sal = '$'+ str(i) + ' - ' + '$' + str(i + 20000)\n",
    "    sal_list.append(sal)\n",
    "\n",
    "# Start looping through the job categories list\n",
    "final_list = []\n",
    "for i in ttl_list:\n",
    "    wd.get(url)\n",
    "    ttl = wd.find_element_by_id('as_ttl')\n",
    "    ttl.clear()\n",
    "    ttl.send_keys(ttl_list[i])     \n",
    "    \n",
    "    cat_holding_list = []\n",
    "    for j in sal_list:\n",
    "        sal = wd.find_element_by_id('salary')\n",
    "        sal.clear()\n",
    "        sal.send_keys(sal_list[j])\n",
    "\n",
    "        # define salary as the average of the salary range. This will serve as target for regression later\n",
    "        sals = re.findall('[\\d]+', sal_list[j])\n",
    "        salary = np.mean([int(x) for x in sals])\n",
    "\n",
    "\n",
    "\n",
    "        form = wd.find_element_by_tag_name('form')\n",
    "        form.submit()\n",
    "\n",
    "        html_page = wd.page_source\n",
    "        soup = bs(html_page, 'html.parser')\n",
    "\n",
    "        # Find urls, if any, for subsequent pages and store them in links\n",
    "        pagelist = soup.find('div', {'class' : 'pagination'})\n",
    "\n",
    "        if pagelist:\n",
    "            urls = pagelist.find_all('a')\n",
    "\n",
    "            links = ['frontpage']\n",
    "            for link in urls:\n",
    "                links.append(link.get('href'))\n",
    "\n",
    "        else:\n",
    "            links = ['frontpage']\n",
    "\n",
    "        sal_holding_list = []\n",
    "\n",
    "        # Looping through pages for a specific salary range\n",
    "        for link in links:\n",
    "\n",
    "            if link == 'frontpage':\n",
    "                soup = bs(html_page, 'html.parser')\n",
    "\n",
    "            else:\n",
    "                wd.get('https://www.indeed.com' + link)\n",
    "                html_page = wd.page_source\n",
    "                soup = bs(html_page, 'html.parser')\n",
    "\n",
    "            # Find the job title details, hiring company, location and job description\n",
    "            joblist = soup.find('td', {'id' : 'resultsCol'})\n",
    "            try:\n",
    "                # some pages may not have jobs in the salary range\n",
    "                jobs = joblist.find_all('div', {'class' : 'row'})\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            job_details = []\n",
    "\n",
    "            for job in jobs:\n",
    "\n",
    "                details = job.find('h2', {'class' : 'jobtitle'})\n",
    "                # find job title  \n",
    "                try:\n",
    "                    job_title = details.find('a').get('title').lower()\n",
    "                    # clean title using textacy.preprocess_text\n",
    "                    clean_title = preprocess_text(job_title, fix_unicode=True, lowercase=True, transliterate=False, \n",
    "                                                no_numbers=True, no_urls=True, no_emails=False, no_phone_numbers=False, \n",
    "                                                no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # find company name\n",
    "                try:\n",
    "                    company = job.find('span', {'class' : 'company'}).find('a').text.strip().lower()\n",
    "                except: \n",
    "                    continue\n",
    "\n",
    "                # find location\n",
    "                try:\n",
    "                    location = job.find('span', {'class' : 'location'}).text.strip().lower()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # find job description\n",
    "                href = details.find('a').get('href')\n",
    "                wd.get('https://www.indeed.com' + href)\n",
    "                page = wd.page_source\n",
    "                soupy = bs(page, 'html.parser')\n",
    "\n",
    "                try:\n",
    "                    job_des = soupy.find('body').find_all('p')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if job_des:\n",
    "\n",
    "                    descrps = []\n",
    "                    for des in job_des:\n",
    "                        if des.text:\n",
    "                            descrps.append(des.text.strip().lower())\n",
    "                    job_descrip = ' '.join(descrps)    \n",
    "\n",
    "                    # clean job description using textacy.preprocess_text\n",
    "                    clean_descrip = preprocess_text(job_descrip, fix_unicode=False, lowercase=True, transliterate=False, \n",
    "                                            no_numbers=True, no_urls=True, no_emails=True, no_phone_numbers=True, \n",
    "                                            no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "                    # check if content is long enough, else likely to be some form of notice\n",
    "                    if len(clean_descrip) > 2000:\n",
    "                        job_details.append([ttl_list[0], job_title, company, location, clean_descrip, salary])\n",
    "\n",
    "            sal_holding_list.extend(job_details)\n",
    "\n",
    "        sal_holding_list[:10]  \n",
    "\n",
    "end = time.time()\n",
    "exe_time = (end - start)/3600\n",
    "print 'Time taken :',(exe_time),' hours'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    }
   ],
   "source": [
    "print len(sal_holding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
