{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport urllib\\nresponse = urllib.urlopen('https://www.misstamchiak.com/eat/').read()\\nsoup = bs(response, 'html.parser')\\n\\nprint soup.prettify()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import urllib\n",
    "response = urllib.urlopen('https://www.misstamchiak.com/eat/').read()\n",
    "soup = bs(response, 'html.parser')\n",
    "\n",
    "print soup.prettify()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 51.9200491905  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "b_summary = []\n",
    "b_detail = []\n",
    "b_date = []\n",
    "for i in range(1,30):\n",
    "    url = 'https://www.misstamchiak.com/eat/page/' + str(i) + '/'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "    \n",
    "    blogs = soup.find_all('h3', {'class' : 'entry-title'})\n",
    "\n",
    "    for summary in blogs:\n",
    "        href = summary.find('a').get('href')\n",
    "        r = requests.get(href)\n",
    "        s = bs(r.text, 'html.parser')\n",
    "\n",
    "        # find date\n",
    "        date = s.find('time', {'class' : 'entry-date updated'}).text\n",
    "\n",
    "        # find detail\n",
    "        y = s.find('div', {'class' : \"tve-tl-cnt-wrap\"}).find_all('p')\n",
    "        doc = []\n",
    "        for p in y:\n",
    "            if p.text:\n",
    "                doc.append(p.text.lower())\n",
    "        d = ' '.join(doc)\n",
    "\n",
    "        b_summary.append(summary.text)\n",
    "        b_detail.append(d)\n",
    "        b_date.append(date)\n",
    "\n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# parse html elements\n",
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "\n",
    "no_html = []\n",
    "for i in b_detail:\n",
    "    i = html_parser.unescape(i)\n",
    "    no_html.append(i)\n",
    "    \n",
    "print len(no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "# remove urls \n",
    "url_re = 'https:\\/\\/www\\.([a-zA-Z\\d]+)\\.com\\/([a-zA-Z\\d]+)\\/([a-zA-Z\\d]*)\\/?'\n",
    "    \n",
    "no_url = []\n",
    "for i in no_html:\n",
    "    u = re.finditer(url_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    # remove newlines including those created when url removed at end of string\n",
    "    i = i.replace('\\n', '')\n",
    "    no_url.append(i)\n",
    "    \n",
    "print len(no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove any screen names encountered\n",
    "no_scrname = []\n",
    "scrname_re = '@([a-zA-Z\\d]*)'\n",
    "for i in no_url:\n",
    "    u = re.finditer(scrname_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_scrname.append(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all hashtags\n",
    "no_hashtag = []\n",
    "hashtag_re = '#([a-z]+)'\n",
    "\n",
    "for i in no_scrname:\n",
    "    u = re.finditer(hashtag_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_hashtag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_spec = []\n",
    "spec_char = ['$', '(', ')','{','}','~','@','#','%','^','&','*',':','|','<',';','[',']','+','!','?','`']\n",
    "\n",
    "for i in no_hashtag:\n",
    "    for char in spec_char:\n",
    "        i = i.replace(char, '')\n",
    "    no_spec.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_num = []\n",
    "num_re = '\\d'\n",
    "\n",
    "for i in no_spec:\n",
    "    u = re.findall(num_re, i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a , '')\n",
    "    no_num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get all a-z characters only and remove emojis, punctuations and special characters\n",
    "char_only = []\n",
    "not_char_re = '([^a-z\\s]+)'\n",
    "\n",
    "for i in no_num:\n",
    "    u = re.findall(not_char_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a , '')\n",
    "    char_only.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create final list to fit in dataframe\n",
    "flist = []\n",
    "for i in range(len(b_summary)):\n",
    "    f = [b_summary[i], char_only[i], b_date[i]]\n",
    "    flist.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Details</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Lobster Time – Slurping Good Time in Bishan!</td>\n",
       "      <td>love lobsters at a small coffeeshop in bishan...</td>\n",
       "      <td>May 12th, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Le Castella – We Waited 5 Hours for Taiwan’s P...</td>\n",
       "      <td>photos by nathanael chan videos of castella a ...</td>\n",
       "      <td>May 12th, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Jenny’s Laksa – Authentic, Tasty and Affordabl...</td>\n",
       "      <td>established in october last year jennys laksa ...</td>\n",
       "      <td>May 11th, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Don Signature Crab – Delicious Crab Beehoon in...</td>\n",
       "      <td>don is wellknown for his pie and crab beehoon ...</td>\n",
       "      <td>May 11th, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Satisfy Your Sweet Tooth With 16 Sweet Treats ...</td>\n",
       "      <td>ours is a nation of foodobsessed people we end...</td>\n",
       "      <td>May 10th, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "314       Lobster Time – Slurping Good Time in Bishan!   \n",
       "315  Le Castella – We Waited 5 Hours for Taiwan’s P...   \n",
       "316  Jenny’s Laksa – Authentic, Tasty and Affordabl...   \n",
       "317  Don Signature Crab – Delicious Crab Beehoon in...   \n",
       "318  Satisfy Your Sweet Tooth With 16 Sweet Treats ...   \n",
       "\n",
       "                                               Details            Date  \n",
       "314   love lobsters at a small coffeeshop in bishan...  May 12th, 2017  \n",
       "315  photos by nathanael chan videos of castella a ...  May 12th, 2017  \n",
       "316  established in october last year jennys laksa ...  May 11th, 2017  \n",
       "317  don is wellknown for his pie and crab beehoon ...  May 11th, 2017  \n",
       "318  ours is a nation of foodobsessed people we end...  May 10th, 2017  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(flist, columns = ['Title', 'Details', 'Date'])  \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if youre from nus youve probably treated yourself to an indulgent meal at waa cow more than once specialising in aburi wagyu beef donburis waa cow has garnered an impressive following since their humble beginning in nus we spoke to one of the  founders of waa cow aaron and he told us that quality is of paramount importance to them their beef is imported from snake river farms in usa and mayura station full blood  we got their classic regular aburi wagyu beef  and chose the truffle wagyu beef option + the addon gets you black truffle pure and white truffle oil drizzled over the bed of rice the aroma of truffle oil hit us even before the bowl was set down on our table their signature donburi consists of premium wagyu beef that sat in a water bath for  hours housemade wagyu beef bone sauce pickles and a wobbly onsen egg topped with tobiko the beef was tender and done just right owing to the fats the beef just melts in your mouth you can add mentaiko  and uni  to your bowl as well if youre a small eater or simply watching your diet go for their petite bowl  you get the same goodness at a fraction of the portion and the price however the petite bowl only comes with the option of adding mentaiko  we recommend splurging on the regularsized bowl being a fan of seafood i was excited to try waacows version of mentaiko salmon don also this item is exclusive to the raffles xchange outlet their mentaiko salmon don  can be spruced up further with the addition of an onsen egg and tobiko  the sashimi grade salmon fillet is done to medium rare before it is slathered with a thick layer of mentaiko sauce the salmon fillet is brushed with aspecial shoyu glaze before it is pan fried the salmon is huge and well worth the price we liked that the insides were still flaky and moist by the way they use yamagata rice and real ground japanese wasabi in all their bowls another item thats exclusive to their sushi bar menu is the yuzukosho iberico butadon  which sees sliced iberico pork simmered in an umami broth their yuzu kosho chilli peppers is imported from fukuoka japan we were really excited for the wagyu beef sushi that this branch is supposed to specialise in we tried the mentaiko wagyu and yuzu kosho wagyu both were torched to medium rare tataki style we were told to have the yuzu kosho wagyu first we felt the spike of the tangy yuzu first before feeling the subtle burn of the yuzu kosho it was actually pretty manageable in terms of spice level next we had the mentaiko wagyu  torched pollock fish roe a slice of wagyu beef sushi rice the decadent creaminess of the mentaiko overwhelmed the beef so we preferred the yuzu kosho wagyu nigiri if youre craving for some quality nosh after work this cosy eatery might just be the place youre looking for address  raffles place  raffles xchange singapore  opening hours am to pm pm to pm monday to friday pm to pm saturday closed on sundays misstamchiakcom made anonymous visit and paid its own meal at the stall featured here lets build a food community that helps to update the food news in singapore simply comment below if theres any changes or additional info to coocaa we will verify and update from our side thanks in advance\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[3,1]\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to csv file\n",
    "with open ('tamchiak.csv', 'w') as tc:\n",
    "    df.to_csv(tc, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
