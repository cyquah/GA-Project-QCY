{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Link to amazon_food.csv\n",
    "# https://drive.google.com/open?id=1J5GDMYqSmxc8z4FaloZ-tx3Le7n43iMg\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just another flavor of Kit Kat but the taste i...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I bought this on impulse and it comes from Jap...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good. Great gift for any fan of green t...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I had never had it before, was curious to see ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been looking forward to trying these afte...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  Just another flavor of Kit Kat but the taste i...      4.0\n",
       "1  I bought this on impulse and it comes from Jap...      3.0\n",
       "2  Really good. Great gift for any fan of green t...      4.0\n",
       "3  I had never had it before, was curious to see ...      5.0\n",
       "4  I've been looking forward to trying these afte...      4.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding to utf-8 here is important \n",
    "df = pd.read_csv('amazon_food.csv', index_col=0, encoding = 'utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151254"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151254\n"
     ]
    }
   ],
   "source": [
    "# parse html elements\n",
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "\n",
    "no_html = []\n",
    "for i in df.reviewText:\n",
    "    i = html_parser.unescape(str(i))\n",
    "    no_html.append(i.lower())\n",
    "    \n",
    "print len(no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151254\n"
     ]
    }
   ],
   "source": [
    "# remove urls \n",
    "url_re = 'https:\\/\\/www\\.([a-zA-Z\\d]+)\\.com\\/([a-zA-Z\\d]+)\\/([a-zA-Z\\d]*)\\/?'\n",
    "    \n",
    "no_url = []\n",
    "for i in no_html:\n",
    "    u = re.finditer(url_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    # remove newlines including those created when url removed at end of string\n",
    "    i = i.replace('\\n', '')\n",
    "    no_url.append(i)\n",
    "    \n",
    "print len(no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove any screen names encountered\n",
    "no_scrname = []\n",
    "scrname_re = '@([a-zA-Z\\d]*)'\n",
    "for i in no_url:\n",
    "    u = re.finditer(scrname_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_scrname.append(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all hashtags\n",
    "no_hashtag = []\n",
    "hashtag_re = '#([a-z]+)'\n",
    "\n",
    "for i in no_scrname:\n",
    "    u = re.finditer(hashtag_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_hashtag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_spec = []\n",
    "spec_char = ['$', '(', ')','{','}','~','@','#','%','^','&','*',':','|','<',';','[',']','+','!','?','`']\n",
    "\n",
    "for i in no_hashtag:\n",
    "    for char in spec_char:\n",
    "        i = i.replace(char, '')\n",
    "    no_spec.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_num = []\n",
    "num_re = '\\d'\n",
    "\n",
    "for i in no_spec:\n",
    "    u = re.findall(num_re, i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a , '')\n",
    "    no_num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151254"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all a-z characters only and remove emojis, punctuations and special characters\n",
    "char_only = []\n",
    "not_char_re = '([^a-z\\s]+)'\n",
    "\n",
    "for i in no_num:\n",
    "    u = re.findall(not_char_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a , '')\n",
    "    char_only.append(i)\n",
    "    \n",
    "len(char_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i had never had it before was curious to see what it was like smooth great subtle good flavor i am ordering more and plan to make it a routine\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[3,0]\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 98.0313000679  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lemma_check =[]\n",
    "wnl = WordNetLemmatizer()\n",
    "for item in char_only:\n",
    "    #item = str(item).decode('utf-8')\n",
    "    t = \" \".join([wnl.lemmatize(i) for i in item.split()])\n",
    "    lemma_check.append(t) \n",
    "    \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431\n"
     ]
    }
   ],
   "source": [
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\", \"singapore\", \"food\", \"im\", \"street\",\"'ve\",\"'re\", \n",
    "                                            'porn','watch', 'video', 'centre', '0', '...', ':',',','.','!','/','(',')',\n",
    "                                             '-','&','`','~','@','#','$','%','^','*','[',']','?','\\\\','{','}',';',\"'\",\n",
    "                                             '\"','+','=',\n",
    "                                             'eat', 'day', 'time', 'cdataadsbygoogle', 'windowadsbygoogle',\n",
    "                                             'wa', 'ha', 'come', 'place', 'dish', 'bring', 'think', 'quite','located',\n",
    "                                             'month', 'went', 'probably','pm', 'say', 'said','including','year','item',\n",
    "                                            'youre', 'sure', 'dont', 'came','really', 'got', 'thing', 'address', 'photo',\n",
    "                                            'credit', 'opening', 'hour'] \n",
    "               + list(ENGLISH_STOP_WORDS))\n",
    "print len(STOPLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 67.7756071091  seconds\n",
      "151254\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "tokens = []\n",
    "for item in lemma_check:  \n",
    "    try:\n",
    "        Tokens = nltk.word_tokenize(item.lower())\n",
    "        t = [tok for tok in Tokens if tok not in STOPLIST]\n",
    "        # print t\n",
    "    except:\n",
    "        continue\n",
    "    tokens.append(t)\n",
    "    \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'\n",
    "    \n",
    "print len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.Series(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [flavor, kit, kat, taste, unique, bit, differe...\n",
       "1    [bought, impulse, japan, amused, family, weird...\n",
       "2    [good, great, gift, fan, green, tea, expensive...\n",
       "3    [curious, like, smooth, great, subtle, good, f...\n",
       "4    [ive, looking, forward, trying, hearing, popul...\n",
       "5    [kitkats, good, looking, strong, green, tea, f...\n",
       "6    [mitsuwa, marketplace, illinoisi, actually, ex...\n",
       "7    [creamy, white, chocolate, infused, matcha, gr...\n",
       "8    [hearing, mixed, opinion, kit, kat, decided, t...\n",
       "9    [love, green, tea, love, kit, kat, belong, hat...\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText_tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>just another flavor of kit kat but the taste i...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[flavor, kit, kat, taste, unique, bit, differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i bought this on impulse and it comes from jap...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>[bought, impulse, japan, amused, family, weird...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>really good great gift for any fan of green te...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[good, great, gift, fan, green, tea, expensive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i had never had it before was curious to see w...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[curious, like, smooth, great, subtle, good, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ive been looking forward to trying these after...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[ive, looking, forward, trying, hearing, popul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall  \\\n",
       "0  just another flavor of kit kat but the taste i...      4.0   \n",
       "1  i bought this on impulse and it comes from jap...      3.0   \n",
       "2  really good great gift for any fan of green te...      4.0   \n",
       "3  i had never had it before was curious to see w...      5.0   \n",
       "4  ive been looking forward to trying these after...      4.0   \n",
       "\n",
       "                                 reviewText_tokenize  \n",
       "0  [flavor, kit, kat, taste, unique, bit, differe...  \n",
       "1  [bought, impulse, japan, amused, family, weird...  \n",
       "2  [good, great, gift, fan, green, tea, expensive...  \n",
       "3  [curious, like, smooth, great, subtle, good, f...  \n",
       "4  [ive, looking, forward, trying, hearing, popul...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewText_tokenize'] = tokens\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('amazon_food-tokenize.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
