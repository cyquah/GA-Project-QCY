{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.width = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'yCX0HfM3kJFmanBlHRqt0T1Of'\n",
    "consumer_secret = 'i5i1KS42ueAklCKQBAr5Fh3yAMUzwkR5nmhIqjcRFXprsEpAWj'\n",
    "access_token = '867963177673269248-3C3LGmQablQZKdaM39NI0zdiPFYEhbZ'\n",
    "access_secret = 'WPW5JhGYkWNqRYgCKUJKcvetdemDJeN8Lt1GL4NP650ec'\n",
    "\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 10000 tweets\n",
      "Downloaded 98 tweets\n",
      "Downloaded 198 tweets\n",
      "Downloaded 298 tweets\n",
      "Downloaded 398 tweets\n",
      "Downloaded 498 tweets\n",
      "Downloaded 598 tweets\n",
      "Downloaded 698 tweets\n",
      "Downloaded 798 tweets\n",
      "Downloaded 882 tweets\n",
      "Downloaded 982 tweets\n",
      "Downloaded 1081 tweets\n",
      "Downloaded 1181 tweets\n",
      "Downloaded 1269 tweets\n",
      "Downloaded 1365 tweets\n",
      "Downloaded 1465 tweets\n",
      "Downloaded 1562 tweets\n",
      "Downloaded 1662 tweets\n",
      "Downloaded 1762 tweets\n",
      "Downloaded 1765 tweets\n",
      "No more tweets found\n",
      "Time taken : 23.7136249542  seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "                                \n",
    "import sys\n",
    "import os\n",
    "\n",
    "searchQuery = 'singapore food'  # this is what we're searching for\n",
    "maxTweets = 10000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1L\n",
    "\n",
    "tweets = []\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "while tweetCount < maxTweets:\n",
    "    try:\n",
    "        if (max_id <= 0):\n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "            else:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        since_id=sinceId)\n",
    "        else:\n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        max_id=str(max_id - 1))\n",
    "            else:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        max_id=str(max_id - 1),\n",
    "                                        since_id=sinceId)\n",
    "        if not new_tweets:\n",
    "            print(\"No more tweets found\")\n",
    "            break\n",
    "            \n",
    "        for tweet in new_tweets:\n",
    "            tweets.append(tweet)\n",
    "\n",
    "        tweetCount += len(new_tweets)\n",
    "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "        \n",
    "        max_id = new_tweets[-1].id\n",
    "    \n",
    "    except tweepy.TweepError as e:\n",
    "        # Just exit if any error\n",
    "        print(\"some error : \" + str(e))\n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1765\n"
     ]
    }
   ],
   "source": [
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355\n",
      "917459255720103936\n"
     ]
    }
   ],
   "source": [
    "tweet_id = []\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "# if tweet.author.location == \"Singapore\":\n",
    "    if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "        results.append(tweet.text.lower())\n",
    "        tweet_id.append(tweet.id)\n",
    "    \n",
    "print len(results)\n",
    "print tweet_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n"
     ]
    }
   ],
   "source": [
    "def no_duplicates(results):\n",
    "    # to remove duplicate tweets that are not retweets by comparing the first 25 characters of the tweet\n",
    "\n",
    "    no_dupli = []\n",
    "    # initialise empty list to contain duplicate tweets once only \n",
    "    dup_once = []\n",
    "    # initialise dup_once ist to contain truncated tweets for comparison\n",
    "    dup_once_trunc = []\n",
    "    res_trunc = [x[:25] for x in results]\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        # remove truncated item to see if there are other duplicates in the list\n",
    "        del res_trunc[i]\n",
    "        if results[i][:25] not in res_trunc:\n",
    "            no_dupli.append(results[i])\n",
    "        else:\n",
    "            # check and input duplicates once only\n",
    "            if results[i][:25] not in dup_once_trunc:\n",
    "                    dup_once.append(results[i])\n",
    "                    dup_once_trunc.append(results[i][:25])\n",
    "        res_trunc = [x[:25] for x in results]\n",
    "\n",
    "    no_dupli.extend(dup_once)\n",
    "            \n",
    "    return no_dupli\n",
    "\n",
    "no_dup = no_duplicates(results)\n",
    "\n",
    "print len(no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n"
     ]
    }
   ],
   "source": [
    "# parse html elements\n",
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "\n",
    "no_html = []\n",
    "for i in no_dup:\n",
    "    i = html_parser.unescape(i)\n",
    "    no_html.append(i)\n",
    "    \n",
    "print len(no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012\n"
     ]
    }
   ],
   "source": [
    "# remove urls at the end of the tweet\n",
    "url_re = 'https:\\/\\/t\\.co\\/([a-zA-Z\\d]*)'\n",
    "    \n",
    "no_url = []\n",
    "for i in no_html:\n",
    "    u = re.finditer(url_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    # remove newline at end of tweet\n",
    "    i = i.replace('\\n', '')\n",
    "    no_url.append(i)\n",
    "    \n",
    "print len(no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove any screen names encountered\n",
    "no_scrname = []\n",
    "scrname_re = '@([a-zA-Z\\d]*)'\n",
    "for i in no_url:\n",
    "    u = re.finditer(scrname_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_scrname.append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all hashtags\n",
    "no_hashtag = []\n",
    "hashtag_re = '#([a-z]+)'\n",
    "\n",
    "for i in no_scrname:\n",
    "    u = re.finditer(hashtag_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_hashtag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get all a-z characters only and remove emojis, numbers, punctuations and special characters\n",
    "char_only = []\n",
    "not_char_re = '([^a-z\\s]+)'\n",
    "for i in no_hashtag:\n",
    "    u = re.finditer(not_char_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    char_only.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>bourdain promises unapologetic food porn for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>from no food at home to dishing out food to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>roundtrip flights to one of asias best food ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>parts unknown kicks off season  with a singapo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>im at jmembina food house in singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>im at mount faber nasi lemak fast food shop  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>months singapore vacanciesrestaurant helperro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>im at food junction in singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>singapore  vacancyrestaurant helperusroom boyu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>ana singapore 羽田空港 ana gourmet delicious food ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweets\n",
       "1002  bourdain promises unapologetic food porn for p...\n",
       "1003  from no food at home to dishing out food to th...\n",
       "1004  roundtrip flights to one of asias best food ci...\n",
       "1005  parts unknown kicks off season  with a singapo...\n",
       "1006            im at jmembina food house in singapore \n",
       "1007  im at mount faber nasi lemak fast food shop  i...\n",
       "1008   months singapore vacanciesrestaurant helperro...\n",
       "1009                  im at food junction in singapore \n",
       "1010  singapore  vacancyrestaurant helperusroom boyu...\n",
       "1011  ana singapore 羽田空港 ana gourmet delicious food ..."
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.DataFrame(char_only, columns = ['Tweets'])\n",
    "tweets.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.to_csv('Tweets.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
