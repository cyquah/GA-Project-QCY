{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.width = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'yCX0HfM3kJFmanBlHRqt0T1Of'\n",
    "consumer_secret = 'i5i1KS42ueAklCKQBAr5Fh3yAMUzwkR5nmhIqjcRFXprsEpAWj'\n",
    "access_token = '867963177673269248-3C3LGmQablQZKdaM39NI0zdiPFYEhbZ'\n",
    "access_secret = 'WPW5JhGYkWNqRYgCKUJKcvetdemDJeN8Lt1GL4NP650ec'\n",
    "\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "                                \n",
    "import sys\n",
    "import os\n",
    "\n",
    "searchQuery = 'food'  # this is what we're searching for\n",
    "maxTweets = 150000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1L\n",
    "\n",
    "tweets = []\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "while tweetCount < maxTweets:\n",
    "    try:\n",
    "        if (max_id <= 0):\n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "            else:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        since_id=sinceId)\n",
    "        else:\n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        max_id=str(max_id - 1))\n",
    "            else:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        max_id=str(max_id - 1),\n",
    "                                        since_id=sinceId)\n",
    "        if not new_tweets:\n",
    "            print(\"No more tweets found\")\n",
    "            break\n",
    "            \n",
    "        for tweet in new_tweets:\n",
    "            tweets.append(tweet)\n",
    "\n",
    "        tweetCount += len(new_tweets)\n",
    "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "        \n",
    "        max_id = new_tweets[-1].id\n",
    "    \n",
    "    except tweepy.TweepError as e:\n",
    "        # Just exit if any error\n",
    "        print(\"some error : \" + str(e))\n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = []\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "# if tweet.author.location == \"Singapore\":\n",
    "    if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "        results.append(tweet.text.lower())\n",
    "        tweet_id.append(tweet.id)\n",
    "    \n",
    "print len(results)\n",
    "print tweet_id[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "def no_duplicates(results):\n",
    "    # to remove duplicate tweets that are not retweets by comparing the first 25 characters of the tweet\n",
    "\n",
    "    no_dupli = []\n",
    "    # initialise empty list to contain duplicate tweets once only \n",
    "    dup_once = []\n",
    "    # initialise dup_once ist to contain truncated tweets for comparison\n",
    "    dup_once_trunc = []\n",
    "    res_trunc = [x[:25] for x in results]\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        # remove truncated item to see if there are other duplicates in the list\n",
    "        del res_trunc[i]\n",
    "        if results[i][:25] not in res_trunc:\n",
    "            no_dupli.append(results[i])\n",
    "        else:\n",
    "            # check and input duplicates once only\n",
    "            if results[i][:25] not in dup_once_trunc:\n",
    "                    dup_once.append(results[i])\n",
    "                    dup_once_trunc.append(results[i][:25])\n",
    "        res_trunc = [x[:25] for x in results]\n",
    "\n",
    "    no_dupli.extend(dup_once)\n",
    "            \n",
    "    return no_dupli\n",
    "\n",
    "no_dup = no_duplicates(results)\n",
    "\n",
    "print len(no_dup)\n",
    "\n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html elements\n",
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "\n",
    "no_html = []\n",
    "for i in no_dup:\n",
    "    i = html_parser.unescape(i)\n",
    "    no_html.append(i)\n",
    "    \n",
    "print len(no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls at the end of the tweet\n",
    "url_re = 'https:\\/\\/t\\.co\\/([a-zA-Z\\d]+)'\n",
    "    \n",
    "no_url = []\n",
    "for i in no_html:\n",
    "    u = re.finditer(url_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    # remove newlines including those created when url removed at end of string\n",
    "    i = i.replace('\\n', '')\n",
    "    no_url.append(i)\n",
    "    \n",
    "print len(no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove any screen names encountered\n",
    "no_scrname = []\n",
    "scrname_re = '@([a-zA-Z\\d]*)'\n",
    "for i in no_url:\n",
    "    u = re.finditer(scrname_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_scrname.append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all hashtags\n",
    "no_hashtag = []\n",
    "hashtag_re = '#([a-z]+)'\n",
    "\n",
    "for i in no_scrname:\n",
    "    u = re.finditer(hashtag_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_hashtag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_spec = []\n",
    "spec_char = ['$', '(', ')']\n",
    "\n",
    "for i in no_hashtag:\n",
    "    for char in spec_char:\n",
    "        i = i.replace(char, '')\n",
    "    no_spec.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_num = []\n",
    "num_re = '\\d'\n",
    "\n",
    "for i in no_spec:\n",
    "    u = re.findall(num_re, i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a , '')\n",
    "    no_num.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get all a-z characters only and remove emojis, punctuations and special characters\n",
    "char_only = []\n",
    "not_char_re = '([^a-z\\s]+)'\n",
    "\n",
    "for i in no_num:\n",
    "    u = re.findall(not_char_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a , '')\n",
    "    char_only.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(char_only, columns = ['Tweets'])\n",
    "tweets.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.to_csv('Tweets150k.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
