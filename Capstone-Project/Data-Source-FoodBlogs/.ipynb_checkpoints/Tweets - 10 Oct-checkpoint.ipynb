{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 50\n",
    "pd.options.display.width = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'yCX0HfM3kJFmanBlHRqt0T1Of'\n",
    "consumer_secret = 'i5i1KS42ueAklCKQBAr5Fh3yAMUzwkR5nmhIqjcRFXprsEpAWj'\n",
    "access_token = '867963177673269248-3C3LGmQablQZKdaM39NI0zdiPFYEhbZ'\n",
    "access_secret = 'WPW5JhGYkWNqRYgCKUJKcvetdemDJeN8Lt1GL4NP650ec'\n",
    "\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 100000 tweets\n",
      "Downloaded 100 tweets\n",
      "Downloaded 200 tweets\n",
      "Downloaded 300 tweets\n",
      "Downloaded 400 tweets\n",
      "Downloaded 500 tweets\n",
      "Downloaded 600 tweets\n",
      "Downloaded 700 tweets\n",
      "Downloaded 800 tweets\n",
      "Downloaded 900 tweets\n",
      "Downloaded 1000 tweets\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "                                \n",
    "import sys\n",
    "import os\n",
    "\n",
    "searchQuery = 'food'  # this is what we're searching for\n",
    "maxTweets = 100000 # Some arbitrary large number\n",
    "tweetsPerQry = 100  # this is the max the API permits\n",
    "\n",
    "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "# else default to no lower limit, go as far back as API allows\n",
    "sinceId = None\n",
    "\n",
    "# If results only below a specific ID are, set max_id to that ID.\n",
    "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "max_id = -1L\n",
    "\n",
    "tweets = []\n",
    "tweetCount = 0\n",
    "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "\n",
    "while tweetCount < maxTweets:\n",
    "    try:\n",
    "        if (max_id <= 0):\n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry)\n",
    "            else:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        since_id=sinceId)\n",
    "        else:\n",
    "            if (not sinceId):\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        max_id=str(max_id - 1))\n",
    "            else:\n",
    "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
    "                                        max_id=str(max_id - 1),\n",
    "                                        since_id=sinceId)\n",
    "        if not new_tweets:\n",
    "            print(\"No more tweets found\")\n",
    "            break\n",
    "            \n",
    "        for tweet in new_tweets:\n",
    "            tweets.append(tweet)\n",
    "\n",
    "        tweetCount += len(new_tweets)\n",
    "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "        \n",
    "        max_id = new_tweets[-1].id\n",
    "    \n",
    "    except tweepy.TweepError as e:\n",
    "        # Just exit if any error\n",
    "        print(\"some error : \" + str(e))\n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = []\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "# if tweet.author.location == \"Singapore\":\n",
    "    if (not tweet.retweeted) and ('RT @' not in tweet.text):\n",
    "        results.append(tweet.text.lower())\n",
    "        tweet_id.append(tweet.id)\n",
    "    \n",
    "print len(results)\n",
    "print tweet_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def no_duplicates(results):\n",
    "    # to remove duplicate tweets that are not retweets by comparing the first 25 characters of the tweet\n",
    "\n",
    "    no_dupli = []\n",
    "    # initialise empty list to contain duplicate tweets once only \n",
    "    dup_once = []\n",
    "    # initialise dup_once ist to contain truncated tweets for comparison\n",
    "    dup_once_trunc = []\n",
    "    res_trunc = [x[:25] for x in results]\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        # remove truncated item to see if there are other duplicates in the list\n",
    "        del res_trunc[i]\n",
    "        if results[i][:25] not in res_trunc:\n",
    "            no_dupli.append(results[i])\n",
    "        else:\n",
    "            # check and input duplicates once only\n",
    "            if results[i][:25] not in dup_once_trunc:\n",
    "                    dup_once.append(results[i])\n",
    "                    dup_once_trunc.append(results[i][:25])\n",
    "        res_trunc = [x[:25] for x in results]\n",
    "\n",
    "    no_dupli.extend(dup_once)\n",
    "            \n",
    "    return no_dupli\n",
    "\n",
    "no_dup = no_duplicates(results)\n",
    "\n",
    "print len(no_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse html elements\n",
    "import HTMLParser\n",
    "html_parser = HTMLParser.HTMLParser()\n",
    "\n",
    "no_html = []\n",
    "for i in no_dup:\n",
    "    i = html_parser.unescape(i)\n",
    "    no_html.append(i)\n",
    "    \n",
    "print len(no_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove urls at the end of the tweet\n",
    "url_re = 'https:\\/\\/t\\.co\\/([a-zA-Z\\d]*)'\n",
    "    \n",
    "no_url = []\n",
    "for i in no_html:\n",
    "    u = re.finditer(url_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    # remove newline at end of tweet\n",
    "    i = i.replace('\\n', '')\n",
    "    no_url.append(i)\n",
    "    \n",
    "print len(no_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove any screen names encountered\n",
    "no_scrname = []\n",
    "scrname_re = '@([a-zA-Z\\d]*)'\n",
    "for i in no_url:\n",
    "    u = re.finditer(scrname_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_scrname.append(i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove all hashtags\n",
    "no_hashtag = []\n",
    "hashtag_re = '#([a-z]+)'\n",
    "\n",
    "for i in no_scrname:\n",
    "    u = re.finditer(hashtag_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    no_hashtag.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get all a-z characters only and remove emojis, numbers, punctuations and special characters\n",
    "char_only = []\n",
    "not_char_re = '([^a-z\\s]+)'\n",
    "for i in no_scrname:\n",
    "    u = re.finditer(not_char_re , i)\n",
    "    if u:\n",
    "        for a in u:\n",
    "            i = i.replace(a.group(), '')\n",
    "    char_only.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(char_only, columns = ['Tweets'])\n",
    "tweets.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets.to_csv('Tweets.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
