{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from selenium import webdriver\n",
    "from textacy.preprocess import preprocess_text\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping page :  1\n",
      "href :  http://eatbook.sg/new-generation-hawkers/\n",
      "cleaned detail snippet :  gone are the days where the hashtag foodporn links you to photos of food fr\n",
      "href :  http://eatbook.sg/upot/\n",
      "cleaned detail snippet :  i doubt im the only one who understands the feeling of being caught up in t\n",
      "href :  http://eatbook.sg/cheap-cai-png/\n",
      "cleaned detail snippet :  my love for cai png was born out of necessity it is affordable has a wide v\n",
      "href :  http://eatbook.sg/b-burger/\n",
      "cleaned detail snippet :  using only fresh handrolled beef patties that are never frozen and only coo\n",
      "href :  http://eatbook.sg/don-and-udon/\n",
      "cleaned detail snippet :  japanese cuisine may be my favourite but holeinthewall joints im familiar w\n",
      "href :  http://eatbook.sg/bedok-hawker/\n",
      "cleaned detail snippet :  from caribbean cuisine to raclette cheese a slew of bedok hawker stalls are\n",
      "href :  http://eatbook.sg/upeh-cafe/\n",
      "cleaned detail snippet :  singapores food scene is constantly changing and so are our taste buds with\n",
      "href :  http://eatbook.sg/underrated-bubble-tea/\n",
      "cleaned detail snippet :  if theres one food trend that has never faded in singapore its bubble tea i\n",
      "href :  http://eatbook.sg/bebek-goreng/\n",
      "cleaned detail snippet :  as embarrassing as it sounds i have never tasted duck meat before in my ent\n",
      "href :  http://eatbook.sg/japanese-hawker-stalls/\n",
      "cleaned detail snippet :  japanese cuisine is my favourite but it often torments me when a yearning h\n",
      "scrapping page :  2\n",
      "href :  http://eatbook.sg/bettership/\n",
      "cleaned detail snippet :  the first time i fell in love with chirashi don was when i had it at sushir\n",
      "href :  http://eatbook.sg/dine-inn/\n",
      "cleaned detail snippet :  awkward that was the first thought that popped into my head when i heard ab\n",
      "href :  http://eatbook.sg/penang-place/\n",
      "cleaned detail snippet :  my parents are both from malaysia and even though our family has been stayi\n",
      "href :  http://eatbook.sg/giant-seafood-dishes/\n",
      "cleaned detail snippet :  whether its sharing a plate of fried hokkien mee or having steamboat with y\n",
      "href :  http://eatbook.sg/makan-makan/\n",
      "cleaned detail snippet :  whenever i think of punggol i have this idea that its an ulu place with not\n",
      "href :  http://eatbook.sg/free-fries/\n",
      "cleaned detail snippet :  update article has been edited to reflect the latest timing 3pm to 6pm for \n",
      "href :  http://eatbook.sg/ridhuans-muslim-delights/\n",
      "cleaned detail snippet :  finding a halal zi char stall in singapore is like finding a needle in a ha\n",
      "href :  http://eatbook.sg/jurong-west-hawker-centre/\n",
      "cleaned detail snippet :  hooray to us westies for the new jurong west hawker centre weve got one mor\n",
      "href :  http://eatbook.sg/sticky-rice/\n",
      "cleaned detail snippet :  the last time i went to bangkok was four years ago and ive been missing the\n",
      "href :  http://eatbook.sg/cheap-smu-food/\n",
      "cleaned detail snippet :  if you are a fellow smugger you would understand the woes of trying to keep\n",
      "Time taken : 2.57373919884  minutes\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "wd = webdriver.Chrome()\n",
    "\n",
    "b_summary = []\n",
    "b_detail = []\n",
    "b_date = []\n",
    "b_summary_clean = []\n",
    "b_detail_clean = []\n",
    "\n",
    "for i in range(1,50):\n",
    "    print 'scrapping page : ', i\n",
    "    url = 'http://eatbook.sg/page/' + str(i) + '/'    \n",
    "\n",
    "    try:\n",
    "        wd.get(url)\n",
    "    except:\n",
    "        continue\n",
    "    response = wd.page_source\n",
    "    soup = bs(response, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        blogs = soup.find_all('div', {'class' : 'post-header'})\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    for summary in blogs:\n",
    "        try:\n",
    "            href = summary.find('h2').find('a').get('href')\n",
    "            print 'href : ', href\n",
    "        except:\n",
    "            continue            \n",
    "        \n",
    "        try:\n",
    "            wd.get(href)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        r = wd.page_source\n",
    "        s = bs(r, 'html.parser')\n",
    "\n",
    "        # find date\n",
    "        try:\n",
    "            date = s.find('span', {'class' : 'date'}).text\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        # find detail\n",
    "        try:\n",
    "            y = s.find('div', {'class' : \"post-entry\"}).find_all('p')\n",
    "        except:\n",
    "            continue\n",
    "        doc = []\n",
    "        for p in y:\n",
    "            if p.text:\n",
    "                doc.append(p.text.lower())\n",
    "        d = ' '.join(doc)\n",
    "\n",
    "        b_summary.append(summary.text)\n",
    "        b_detail.append(d)\n",
    "        b_date.append(date)\n",
    "        \n",
    "        # clean summary and detail \n",
    "        clean_summary = preprocess_text(summary.text, fix_unicode=True, lowercase=True, transliterate=True, \n",
    "                                                no_numbers=True, no_urls=True, no_emails=True, no_phone_numbers=True, \n",
    "                                                no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "        \n",
    "        clean_detail = preprocess_text(d, fix_unicode=True, lowercase=True, transliterate=True, \n",
    "                                                no_numbers=True, no_urls=True, no_emails=True, no_phone_numbers=True, \n",
    "                                                no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "        \n",
    "        b_summary_clean.append(clean_summary)\n",
    "        b_detail_clean.append(clean_detail)\n",
    "        \n",
    "        print 'cleaned detail snippet : ', clean_detail[:75]\n",
    "        \n",
    "end = time.time()\n",
    "exe_time = (end - start)/60\n",
    "print 'Time taken :',(exe_time),' minutes'\n",
    "\n",
    "print len(b_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stop words from cleaned summary and cleaned detail and lemmatize them\n",
    "\n",
    "stoplist = set(stopwords.words('english') + ['number'] + list(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "summary_tokens = []\n",
    "for item in b_summary_clean:  \n",
    "    try:\n",
    "        Tokens = nltk.word_tokenize(item)\n",
    "        t = [tok for tok in Tokens if tok not in stoplist]\n",
    "    except:\n",
    "        continue\n",
    "    summary_tokens.append(t)\n",
    "    \n",
    "detail_tokens = []\n",
    "for item in b_detail_clean:  \n",
    "    try:\n",
    "        Tokens = nltk.word_tokenize(item)\n",
    "        t = [tok for tok in Tokens if tok not in stoplist]\n",
    "    except:\n",
    "        continue\n",
    "    detail_tokens.append(t)   \n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'\n",
    "    \n",
    "print len(summary_tokens)\n",
    "print len(detail_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "summary_lem =[]\n",
    "wnl = WordNetLemmatizer()\n",
    "for item in summary_tokens:\n",
    "    t = \" \".join([wnl.lemmatize(i) for i in item.split()])\n",
    "    summary_lem.append(t) \n",
    "    \n",
    "detail_lem =[]\n",
    "wnl = WordNetLemmatizer()\n",
    "for item in detail_tokens:\n",
    "    t = \" \".join([wnl.lemmatize(i) for i in item.split()])\n",
    "    detail_lem.append(t)     \n",
    "   \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'\n",
    "\n",
    "print len(summary_lem)\n",
    "print len(detail_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create final list to fit in dataframe\n",
    "flist = []\n",
    "for i in range(len(b_summary)):\n",
    "    f = [b_summary[i], b_detail[i], b_date[i], summary_lem[i], detail_lem[i]]\n",
    "    flist.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(flist, columns = ['Title', 'Details', 'Date', 'Processed Title', 'Processed Detail'])  \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df.iloc[3,1]\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle dataframe\n",
    "df.to_pickle(eatbook.pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
