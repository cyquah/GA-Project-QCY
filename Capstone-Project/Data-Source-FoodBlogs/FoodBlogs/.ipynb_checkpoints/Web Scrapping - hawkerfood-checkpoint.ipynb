{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from selenium import webdriver\n",
    "from textacy.preprocess import preprocess_text\n",
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping page :  1\n",
      "href :  http://www.hawkerfood.com/chin-mee-chin-confectionery-katong-east-coast-road/\n",
      "cleaned detail snippet :  i am pretty sure many of you have visited chin mee chin confectionery locat\n",
      "href :  http://www.hawkerfood.com/sheng-seng-fried-prawn-noodle-pek-kio-market-food-centre/\n",
      "cleaned detail snippet :  food prices are always on the rise so it is hard to find something that is \n",
      "href :  http://www.hawkerfood.com/engs-noodles-house-wanton-mee-tanjong-katong/\n",
      "cleaned detail snippet :  a few months back i wandered to tanjong katong to jalanjalan and to look fo\n",
      "href :  http://www.hawkerfood.com/ponggol-nasi-lemak-upper-serangoon-road/\n",
      "cleaned detail snippet :  ponggol or punggol is literally the same word with different spelling but m\n",
      "href :  http://www.hawkerfood.com/armenian-street-char-kway-teow-anchorvale-sengkang/\n",
      "cleaned detail snippet :  armenian street char kway teow ya mi ni ya jie chao guo tiao has a history \n",
      "href :  http://www.hawkerfood.com/indonesian-ayam-bakar-changi-village/\n",
      "cleaned detail snippet :  there are not many stalls in singapore dedicated to just selling ayam bakar\n",
      "href :  http://www.hawkerfood.com/328-katong-laksa-east-coast-road/\n",
      "cleaned detail snippet :  when you mention laksa number katong laksa number jia dong le sha easily co\n",
      "href :  http://www.hawkerfood.com/tsuru-koshi-udon-takashimaya-orchard/\n",
      "cleaned detail snippet :  takashimaya shopping centre basement number is a nice place to find all gen\n",
      "href :  http://www.hawkerfood.com/hong-heng-beef-noodle-soup-laksa-kebun-baru/\n",
      "cleaned detail snippet :  it is a known fact that some hawker stalls in singapore only operate until \n",
      "href :  http://www.hawkerfood.com/international-muslim-food-stall-nasi-lemak-changi-village/\n",
      "cleaned detail snippet :  lunchtime scenario you walk into a hawker centre craving for nasi lemak and\n",
      "Time taken : 1.70363534689  minutes\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "wd = webdriver.Chrome()\n",
    "\n",
    "b_summary = []\n",
    "b_detail = []\n",
    "b_date = []\n",
    "b_summary_clean = []\n",
    "b_detail_clean = []\n",
    "\n",
    "for i in range(1,2):\n",
    "    print 'scrapping page : ', i\n",
    "    url = 'http://www.hawkerfood.com/page/' + str(i) + '/'  \n",
    "\n",
    "    try:\n",
    "        wd.get(url)\n",
    "    except:\n",
    "        continue\n",
    "    response = wd.page_source\n",
    "    soup = bs(response, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        blogs = soup.find_all('div', {'class' : 'blog-item-wrap'})\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    for summary in blogs:\n",
    "        try:\n",
    "            href = summary.find('a').get('href')\n",
    "            print 'href : ', href\n",
    "        except:\n",
    "            continue            \n",
    "        \n",
    "        try:\n",
    "            wd.get(href)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        r = wd.page_source\n",
    "        s = bs(r, 'html.parser')\n",
    "\n",
    "        # find date\n",
    "        try:\n",
    "            date = s.find('time', {'class' : 'entry-date'}).text\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        # find detail\n",
    "        try:\n",
    "            y = s.find('div', {'class' : \"entry-content\"}).find_all('p')\n",
    "        except:\n",
    "            continue\n",
    "        doc = []\n",
    "        for p in y:\n",
    "            if p.text:\n",
    "                doc.append(p.text)\n",
    "        d = ' '.join(doc)\n",
    "\n",
    "        b_summary.append(summary.text)\n",
    "        b_detail.append(d)\n",
    "        b_date.append(date)\n",
    "        \n",
    "        # clean summary and detail \n",
    "        clean_summary = preprocess_text(summary.text, fix_unicode=True, lowercase=True, transliterate=True, \n",
    "                                                no_numbers=True, no_urls=True, no_emails=True, no_phone_numbers=True, \n",
    "                                                no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "        \n",
    "        clean_detail = preprocess_text(d, fix_unicode=True, lowercase=True, transliterate=True, \n",
    "                                                no_numbers=True, no_urls=True, no_emails=True, no_phone_numbers=True, \n",
    "                                                no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "        \n",
    "        b_summary_clean.append(clean_summary)\n",
    "        b_detail_clean.append(clean_detail)\n",
    "        \n",
    "        print 'cleaned detail snippet : ', clean_detail[:75]\n",
    "        \n",
    "end = time.time()\n",
    "exe_time = (end - start)/60\n",
    "print 'Time taken :',(exe_time),' minutes'\n",
    "\n",
    "print len(b_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stop words from cleaned summary and cleaned detail and lemmatize them\n",
    "\n",
    "stoplist = set(stopwords.words('english') + \\\n",
    "               ['number',\"singapore\", \"food\", \"im\", \"street\", \n",
    "                'porn','watch', 'video', 'centre', '...',\n",
    "                'eat', 'day', 'time', 'cdataadsbygoogle', 'windowadsbygoogle',\n",
    "                'wa', 'ha', 'come', 'place', 'dish', 'bring', 'think', 'quite','located',\n",
    "                'month', 'went', 'probably', 'am', 'pm', 'say', 'said','including','year','item',\n",
    "                'youre', 'sure', 'dont', 'came','really', 'got', 'thing', 'address', 'photo',\n",
    "                'credit', 'opening', 'hour'] \\\n",
    "                 + list(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0.101689100266  seconds\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "summary_tokens = []\n",
    "for item in b_summary_clean:  \n",
    "    try:\n",
    "        Tokens = nltk.word_tokenize(item)\n",
    "        t = [tok for tok in Tokens if tok not in stoplist]\n",
    "    except:\n",
    "        continue\n",
    "    summary_tokens.append(t)\n",
    "    \n",
    "detail_tokens = []\n",
    "for item in b_detail_clean:  \n",
    "    try:\n",
    "        Tokens = nltk.word_tokenize(item)\n",
    "        t = [tok for tok in Tokens if tok not in stoplist]\n",
    "    except:\n",
    "        continue\n",
    "    detail_tokens.append(t)   \n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'\n",
    "    \n",
    "print len(summary_tokens)\n",
    "print len(detail_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 3.36823415756  seconds\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "summary_lem =[]\n",
    "wnl = WordNetLemmatizer()\n",
    "for item in summary_tokens:\n",
    "    t = \" \".join([wnl.lemmatize(i) for i in item])\n",
    "    summary_lem.append(t) \n",
    "    \n",
    "detail_lem =[]\n",
    "wnl = WordNetLemmatizer()\n",
    "for item in detail_tokens:\n",
    "    t = \" \".join([wnl.lemmatize(i) for i in item])\n",
    "    detail_lem.append(t)     \n",
    "   \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'\n",
    "\n",
    "print len(summary_lem)\n",
    "print len(detail_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create final list to fit in dataframe\n",
    "flist = []\n",
    "for i in range(len(b_summary)):\n",
    "    f = [b_summary[i], b_detail[i], b_date[i], summary_lem[i], detail_lem[i]]\n",
    "    flist.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Details</th>\n",
       "      <th>Date</th>\n",
       "      <th>Processed Summary</th>\n",
       "      <th>Processed Detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\n \\n\\n\\nIndonesian Ayam Bakar in Changi Vil...</td>\n",
       "      <td>there are not many stalls in singapore dedicat...</td>\n",
       "      <td>May 14, 2017</td>\n",
       "      <td>indonesian ayam bakar changi village hawker la...</td>\n",
       "      <td>stall dedicated selling ayam bakar indonesian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\n \\n\\n\\n328 Katong Laksa (328 加东叻沙) along E...</td>\n",
       "      <td>when you mention laksa, 328 katong laksa (328 ...</td>\n",
       "      <td>April 23, 2017</td>\n",
       "      <td>katong laksa jia dong le sha east coast road a...</td>\n",
       "      <td>mention laksa katong laksa jia dong le sha eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n\\n \\n\\n\\nTsuru-Koshi Udon in Takashimaya Sho...</td>\n",
       "      <td>takashimaya shopping centre basement 2 is a ni...</td>\n",
       "      <td>March 25, 2017</td>\n",
       "      <td>tsurukoshi udon takashimaya shopping march lao...</td>\n",
       "      <td>takashimaya shopping basement nice genre espec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n \\n\\n\\nHong Heng Beef Noodle Soup and Laks...</td>\n",
       "      <td>it is a known fact that some hawker stalls in ...</td>\n",
       "      <td>February 21, 2017</td>\n",
       "      <td>hong heng beef noodle soup laksa kebun baru fe...</td>\n",
       "      <td>known fact hawker stall operate end lunchtime ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n\\n \\n\\n\\nInternational Muslim Food Stall Nas...</td>\n",
       "      <td>lunchtime scenario: you walk into a hawker cen...</td>\n",
       "      <td>January 18, 2017</td>\n",
       "      <td>international muslim stall nasi lemak changi v...</td>\n",
       "      <td>lunchtime scenario walk hawker craving nasi le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "5  \\n\\n \\n\\n\\nIndonesian Ayam Bakar in Changi Vil...   \n",
       "6  \\n\\n \\n\\n\\n328 Katong Laksa (328 加东叻沙) along E...   \n",
       "7  \\n\\n \\n\\n\\nTsuru-Koshi Udon in Takashimaya Sho...   \n",
       "8  \\n\\n \\n\\n\\nHong Heng Beef Noodle Soup and Laks...   \n",
       "9  \\n\\n \\n\\n\\nInternational Muslim Food Stall Nas...   \n",
       "\n",
       "                                             Details               Date  \\\n",
       "5  there are not many stalls in singapore dedicat...       May 14, 2017   \n",
       "6  when you mention laksa, 328 katong laksa (328 ...     April 23, 2017   \n",
       "7  takashimaya shopping centre basement 2 is a ni...     March 25, 2017   \n",
       "8  it is a known fact that some hawker stalls in ...  February 21, 2017   \n",
       "9  lunchtime scenario: you walk into a hawker cen...   January 18, 2017   \n",
       "\n",
       "                                   Processed Summary  \\\n",
       "5  indonesian ayam bakar changi village hawker la...   \n",
       "6  katong laksa jia dong le sha east coast road a...   \n",
       "7  tsurukoshi udon takashimaya shopping march lao...   \n",
       "8  hong heng beef noodle soup laksa kebun baru fe...   \n",
       "9  international muslim stall nasi lemak changi v...   \n",
       "\n",
       "                                    Processed Detail  \n",
       "5  stall dedicated selling ayam bakar indonesian ...  \n",
       "6  mention laksa katong laksa jia dong le sha eas...  \n",
       "7  takashimaya shopping basement nice genre espec...  \n",
       "8  known fact hawker stall operate end lunchtime ...  \n",
       "9  lunchtime scenario walk hawker craving nasi le...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(flist, columns = ['Title', 'Details', 'Date', 'Processed Summary', 'Processed Detail'])  \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ponggol or punggol is literally the same word with different spelling but most of us are more familiar with the latter. thus, when i first saw the name ponggol nasi lemak, i was kind of intrigued. what was even more intriguing to me initially was the long queue for its nasi lemak during peak dinner hours.   to avoid the crowd, i was there right at their opening time of 5.30pm and a queue has already formed. most were ordering takeaways with a handful of them dining in. before i joined the queue, i went to browse the dishes and saw a wide variety of them like ikan bilis, ikan kuning, fried chicken wing, luncheon meat, fish fillet, otah, an assortment of vegetables, etc.   the queue was moving pretty fast and probably because none in front of me made a bulk order which was kind of a relief to me. i ordered curry vegetables, egg, fried chicken wing and three pieces of ikan kuning (yellow-banded scad). as always, the senile uncle in me forgot to take note of the price but i remember it was not cheap at all. the coconut rice was rich and fragrant, mixing it with the chilli only made it more delectable. the fried chicken wing was nicely battered and it was crispy and chewy to the bite.  i also liked the curry vegetables which had a nice spicy gravy soaking the soft but crunchy cabbage slices.   the winner had to be the ikan kuning! the fishes were well marinated with an assortment of spices and fried beautifully giving it a nice crisp and sumptuous taste. i think i can just buy a bunch of them to solely eat as a snack.   lao beng’s verdict tasty nasi lemak with a wide variety of dishes for your selection. do try to go early or go after the peak dinner hours to avoid the long queue. for updated opening hours, other outlets and news, please visit their facebook page and web page here: https://www.facebook.com/ponggolnasilemak/ http://www.ponggolnasilemak.com.sg/index.php/contact/   address ponggol nasi lemak centre (榜鹅椰浆饭中心) operating hours: 5:30 pm – 3:30 am tel: 6281-0020 mee sek food court 965 upper serangoon road singapore 534721 nearest mrt: kovan ne13 (nel – north east line) lunchtime scenario: you walk into a hawker centre craving for nasi lemak, and you see a few nasi lemak stalls all having long queues. which queue will you join? lao beng chose the one with the longest queue as i had time to burn plus the fact that i had… january 18, 2017 in \"food\" it was around 10.30 in the morning and here was lao beng way behind the queue for yi liu xiang (一流香) nasi lemak. lao beng was craving for the kuning fish (yellowstripe scad) for months since my last visit. i was hoping against hope that the kuning fish would still… july 1, 2016 in \"food\" \"ayam penyet\" (smashed fried chicken) is a popular malay hawker food in singapore. lao beng was at chong pang market and food centre after a session of strenuous exercise over at khatib camp for my remedial training. (training for national service men who did not pass their annual ippt -… august 19, 2015 in \"food\"\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[3,1]\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle dataframe\n",
    "df.to_pickle('hawkerfood.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
