{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from selenium import webdriver\n",
    "from textacy.preprocess import preprocess_text\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "wd = webdriver.Chrome()\n",
    "\n",
    "b_summary = []\n",
    "b_detail = []\n",
    "b_date = []\n",
    "b_summary_clean = []\n",
    "b_detail_clean = []\n",
    "\n",
    "for i in range(1,50):\n",
    "    print 'scrapping page : ', i\n",
    "    url = 'http://danielfooddiary.com/page/' + str(i) + '/'    \n",
    "\n",
    "    try:\n",
    "        wd.get(url)\n",
    "    except:\n",
    "        continue\n",
    "    response = wd.page_source\n",
    "    soup = bs(response, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        blogs = soup.find_all('div', {'class' : 'blgtyp2-inner-c'})\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    for summary in blogs:\n",
    "        try:\n",
    "            href = summary.find('a').get('href')\n",
    "            print 'href : ', href\n",
    "        except:\n",
    "            continue            \n",
    "        \n",
    "        try:\n",
    "            wd.get(href)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        r = wd.page_source\n",
    "        s = bs(r, 'html.parser')\n",
    "\n",
    "        # find date\n",
    "        try:\n",
    "            date = s.find('time', {'class' : 'entry-date updated'}).text\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        # find detail\n",
    "        try:\n",
    "            y = s.find('div', {'class' : \"tve-tl-cnt-wrap\"}).find_all('p')\n",
    "        except:\n",
    "            continue\n",
    "        doc = []\n",
    "        for p in y:\n",
    "            if p.text:\n",
    "                doc.append(p.text.lower())\n",
    "        d = ' '.join(doc)\n",
    "\n",
    "        b_summary.append(summary.text)\n",
    "        b_detail.append(d)\n",
    "        b_date.append(date)\n",
    "        \n",
    "        # clean summary and detail \n",
    "        clean_summary = preprocess_text(summary.text, fix_unicode=True, lowercase=True, transliterate=True, \n",
    "                                                no_numbers=True, no_urls=True, no_emails=True, no_phone_numbers=True, \n",
    "                                                no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "        \n",
    "        clean_detail = preprocess_text(d, fix_unicode=True, lowercase=True, transliterate=True, \n",
    "                                                no_numbers=True, no_urls=True, no_emails=True, no_phone_numbers=True, \n",
    "                                                no_currency_symbols=True, no_punct=True, no_accents=True)\n",
    "        \n",
    "        b_summary_clean.append(clean_summary)\n",
    "        b_detail_clean.append(clean_detail)\n",
    "        \n",
    "        print 'cleaned detail snippet : ', clean_detail[:75]\n",
    "        \n",
    "end = time.time()\n",
    "exe_time = (end - start)/60\n",
    "print 'Time taken :',(exe_time),' minutes'\n",
    "\n",
    "print len(b_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stop words from cleaned summary and cleaned detail and lemmatize them\n",
    "\n",
    "stoplist = set(stopwords.words('english') + ['number'] + list(ENGLISH_STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "summary_tokens = []\n",
    "for item in b_summary_clean:  \n",
    "    try:\n",
    "        Tokens = nltk.word_tokenize(item)\n",
    "        t = [tok for tok in Tokens if tok not in stoplist]\n",
    "    except:\n",
    "        continue\n",
    "    summary_tokens.append(t)\n",
    "    \n",
    "detail_tokens = []\n",
    "for item in b_detail_clean:  \n",
    "    try:\n",
    "        Tokens = nltk.word_tokenize(item)\n",
    "        t = [tok for tok in Tokens if tok not in stoplist]\n",
    "    except:\n",
    "        continue\n",
    "    detail_tokens.append(t)   \n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'\n",
    "    \n",
    "print len(summary_tokens)\n",
    "print len(detail_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "summary_lem =[]\n",
    "wnl = WordNetLemmatizer()\n",
    "for item in summary_tokens:\n",
    "    t = \" \".join([wnl.lemmatize(i) for i in item.split()])\n",
    "    summary_lem.append(t) \n",
    "    \n",
    "detail_lem =[]\n",
    "wnl = WordNetLemmatizer()\n",
    "for item in detail_tokens:\n",
    "    t = \" \".join([wnl.lemmatize(i) for i in item.split()])\n",
    "    detail_lem.append(t)     \n",
    "   \n",
    "end = time.time()\n",
    "exe_time = end - start\n",
    "print 'Time taken :',(exe_time),' seconds'\n",
    "\n",
    "print len(summary_lem)\n",
    "print len(detail_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create final list to fit in dataframe\n",
    "flist = []\n",
    "for i in range(len(b_summary)):\n",
    "    f = [b_summary[i], b_detail[i], b_date[i], summary_lem[i], detail_lem[i]]\n",
    "    flist.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(flist, columns = ['Title', 'Details', 'Date', 'Processed Title', 'Processed Detail'])  \n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df.iloc[3,1]\n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle dataframe\n",
    "df.to_pickle(tamchiak.pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
