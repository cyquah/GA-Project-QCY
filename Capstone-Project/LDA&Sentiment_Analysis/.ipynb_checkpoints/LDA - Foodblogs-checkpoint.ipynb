{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foodblogs_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[bak, chor, mee, translates, minced, meat, noo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hokkien, prawn, mee, iconic, variant, penang,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[nu, youve, treated, indulgent, meal, waa, cow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[newly, opened, hawkerman, soontobe, halal, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[economical, bee, hoon, singaporean, favourite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 foodblogs_tokenized\n",
       "0  [bak, chor, mee, translates, minced, meat, noo...\n",
       "1  [hokkien, prawn, mee, iconic, variant, penang,...\n",
       "2  [nu, youve, treated, indulgent, meal, waa, cow...\n",
       "3  [newly, opened, hawkerman, soontobe, halal, ce...\n",
       "4  [economical, bee, hoon, singaporean, favourite..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when read from csv, default is text, need to specify dtype=object to preserve info as tokens\n",
    "txt = pd.read_csv('combined_blogs-tokenize.csv', dtype='object', index_col=0, names=['foodblogs_tokenized'], encoding = 'utf-8')\n",
    "txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document = txt.foodblogs_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "1719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'[bak',\n",
       " u' chor',\n",
       " u' mee',\n",
       " u' translates',\n",
       " u' minced',\n",
       " u' meat',\n",
       " u' noodle',\n",
       " u' noodle',\n",
       " u' tossed',\n",
       " u' vinegar',\n",
       " u' minced',\n",
       " u' meat',\n",
       " u' pork',\n",
       " u' slice',\n",
       " u' pork',\n",
       " u' liver',\n",
       " u' stewed',\n",
       " u' sliced',\n",
       " u' mushroom',\n",
       " u' meat',\n",
       " u' ball',\n",
       " u' bit',\n",
       " u' deepfried',\n",
       " u' lard',\n",
       " u' addition',\n",
       " u' chilli',\n",
       " u' optional',\n",
       " u' choice',\n",
       " u' mee',\n",
       " u' pok',\n",
       " u' flat',\n",
       " u' egg',\n",
       " u' noodle',\n",
       " u' mee',\n",
       " u' kia',\n",
       " u' egg',\n",
       " u' noodle',\n",
       " u' choose',\n",
       " u' dry',\n",
       " u' version',\n",
       " u' soup',\n",
       " u' version',\n",
       " u' traditional',\n",
       " u' hawker',\n",
       " u' add',\n",
       " u' small',\n",
       " u' piece',\n",
       " u' fried',\n",
       " u' crispy',\n",
       " u' sole',\n",
       " u' fish',\n",
       " u' chinese',\n",
       " u' lettuce',\n",
       " u' garnishing',\n",
       " u' collated',\n",
       " u' list',\n",
       " u' singaporean',\n",
       " u' favourite',\n",
       " u' bak',\n",
       " u' chor',\n",
       " u' mee',\n",
       " u' stall',\n",
       " u' owner',\n",
       " u' uncle',\n",
       " u' yap',\n",
       " u' start',\n",
       " u' work',\n",
       " u' morning',\n",
       " u' broth',\n",
       " u' boiled',\n",
       " u' old',\n",
       " u' hen',\n",
       " u' soy',\n",
       " u' bean',\n",
       " u' anchovy',\n",
       " u' bowl',\n",
       " u' rich',\n",
       " u' cloudy',\n",
       " u' soup',\n",
       " u' tasty',\n",
       " u' hint',\n",
       " u' sweetness',\n",
       " u' uncle',\n",
       " u' yap',\n",
       " u' add',\n",
       " u' power',\n",
       " u' braised',\n",
       " u' mushroom',\n",
       " u' sauce',\n",
       " u' noodle',\n",
       " u' serving',\n",
       " u' flavour',\n",
       " u' strong',\n",
       " u' tanginess',\n",
       " u' presence',\n",
       " u' black',\n",
       " u' vinegar',\n",
       " u' read',\n",
       " u' macpherson',\n",
       " u' minced',\n",
       " u' ...]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to convert each token to unicode rather than having entire list as 1 unicode, else Dictionary cannot process\n",
    "texts = [[token for token in text.split(',')] for text in document]\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19185\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
